import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from transformers import pipeline
import os
import numpy as np
from sklearn.decomposition import PCA
from sklearn.feature_extraction.text import TfidfVectorizer
from wordcloud import WordCloud

"""
Bu projede, NLP modellerindeki gizli Ã¶nyargÄ±larÄ± tespit etmek, analiz etmek ve gÃ¶rselleÅŸtirmek amacÄ±yla Crows-Pairs veri seti kullanarak kapsamlÄ± bir Ã§alÄ±ÅŸma gerÃ§ekleÅŸtirdim. 
Veri setini temizleyerek Ã¶n iÅŸleme adÄ±mlarÄ±nÄ± tamamladÄ±m ve eksik veri analizleri yaptÄ±m. Ã–nyargÄ± tÃ¼rlerinin daÄŸÄ±lÄ±mÄ±nÄ± grafiklerle gÃ¶rselleÅŸtirerek detaylÄ± bir analiz gerÃ§ekleÅŸtirdim. 
BERT modeliyle `fill-mask` yÃ¶ntemi kullanarak cÃ¼mle dÃ¼zeyinde Ã¶nyargÄ± deÄŸerlendirmeleri yaptÄ±m ve modelin tahminlerini inceledim. 
Ã–nyargÄ±lÄ± (`sent_more`) ve Ã¶nyargÄ±sÄ±z (`sent_less`) cÃ¼mlelerde TF-IDF yÃ¶ntemiyle kelime frekans analizleri yaparak Ã¶ne Ã§Ä±kan kelimeleri belirledim ve Word Cloud gÃ¶rselleÅŸtirmeleri oluÅŸturdum. 
Stereo/antistereo daÄŸÄ±lÄ±mÄ±nÄ± heatmap kullanarak haritalandÄ±rdÄ±m ve PCA yÃ¶ntemiyle kelime gÃ¶mme uzayÄ±nÄ± 2 boyutlu olarak gÃ¶rselleÅŸtirdim. 
AyrÄ±ca, Ã¶nyargÄ±lÄ± ve Ã¶nyargÄ±sÄ±z cÃ¼mlelerin uzunluklarÄ±nÄ± histogramlarla karÅŸÄ±laÅŸtÄ±rmalÄ± olarak analiz ettim. 
Son olarak, elde ettiÄŸim sonuÃ§larÄ± temizlenmiÅŸ ve analiz edilmiÅŸ bir veri seti olarak kaydederek, NLP modellerindeki Ã¶nyargÄ±larÄ±n anlaÅŸÄ±lmasÄ±na ve azaltÄ±lmasÄ±na katkÄ± saÄŸlamayÄ± hedefledim.
"""

# Veri Seti Yolu ve Ã‡alÄ±ÅŸma Dizini
file_path = '/Users/bilge/Desktop/crows-pairs/data/crows_pairs_anonymized.csv'
os.chdir('/Users/bilge/Desktop/crows-pairs')
print("Ã‡alÄ±ÅŸma dizini:", os.getcwd())

# Veri Setini YÃ¼kle
try:
    df = pd.read_csv(file_path)
    print(f"Toplam satÄ±r sayÄ±sÄ±: {df.shape[0]}, Toplam sÃ¼tun sayÄ±sÄ±: {df.shape[1]}")
    print("SÃ¼tun isimleri:", df.columns)
except FileNotFoundError:
    print(f"Hata: Dosya bulunamadÄ± -> {file_path}")
    exit()

if 'Unnamed: 0' in df.columns:
    df = df.drop(columns=['Unnamed: 0'])

print("\nVeri Seti Ä°lk 5 SatÄ±r:")
print(df.head())

# Ã–nyargÄ± TÃ¼rlerinin DaÄŸÄ±lÄ±mÄ±
bias_distribution = df['bias_type'].value_counts()
print("\nğŸ” Ã–nyargÄ± TÃ¼rlerinin DaÄŸÄ±lÄ±mÄ±:")
print(bias_distribution)

bias_distribution.plot(kind='bar')
plt.title('Ã–nyargÄ± TÃ¼rlerinin DaÄŸÄ±lÄ±mÄ±')
plt.xlabel('Ã–nyargÄ± TÃ¼rÃ¼')
plt.ylabel('Frekans')
plt.show()

print("\n Ã–nyargÄ±lÄ± CÃ¼mle Ã–rnekleri:")
print(df['sent_more'].head(5))

print("\n Ã–nyargÄ±sÄ±z CÃ¼mle Ã–rnekleri:")
print(df['sent_less'].head(5))

# NLP Modeli (BERT) YÃ¼kle - Yerel Dizin
try:
    classifier = pipeline('fill-mask', model='/Users/bilge/.cache/huggingface/hub/models--bert-base-uncased')
    print("\nğŸ¤– BERT Modeli Yerel Dizinden YÃ¼klendi BaÅŸarÄ±yla!")
except Exception as e:
    print(f"Hata: NLP modeli yÃ¼klenirken sorun oluÅŸtu. Detay: {e}")
    exit()

# Ã–nyargÄ±lÄ± ve Ã–nyargÄ±sÄ±z CÃ¼mle Analizi
example_sent_more = df['sent_more'].iloc[0].replace("he", "[MASK]")
example_sent_less = df['sent_less'].iloc[0].replace("she", "[MASK]")

print("\n Ã–nyargÄ±lÄ± CÃ¼mle Analizi:")
result_more = classifier(example_sent_more)
for res in result_more[:5]:
    print(f"{res['sequence']} (Skor: {res['score']:.4f})")

print("\n Ã–nyargÄ±sÄ±z CÃ¼mle Analizi:")
result_less = classifier(example_sent_less)
for res in result_less[:5]:
    print(f"{res['sequence']} (Skor: {res['score']:.4f})")

# Eksik Veri Analizi
print("\n Eksik Veri Analizi:")
print(df.isnull().sum())

plt.figure(figsize=(10, 6))
df['sent_more'].str.len().plot(kind='hist', bins=30, alpha=0.7, label='Ã–nyargÄ±lÄ± CÃ¼mle')
df['sent_less'].str.len().plot(kind='hist', bins=30, alpha=0.7, label='Ã–nyargÄ±sÄ±z CÃ¼mle')
plt.title('Ã–nyargÄ±lÄ± ve Ã–nyargÄ±sÄ±z CÃ¼mle UzunluklarÄ± DaÄŸÄ±lÄ±mÄ±')
plt.xlabel('CÃ¼mle UzunluÄŸu')
plt.ylabel('Frekans')
plt.legend()
plt.show()

# Ã–nyargÄ± TÃ¼rlerinin HaritalandÄ±rÄ±lmasÄ±
plt.figure(figsize=(12, 8))
sns.heatmap(pd.crosstab(df['bias_type'], df['stereo_antistereo']), annot=True, fmt='d', cmap='coolwarm')
plt.title('Ã–nyargÄ± TÃ¼rleri ve Stereo/Antistereo DaÄŸÄ±lÄ±mÄ±')
plt.show()

# TF-IDF Kelime Analizi
tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=50)
tfidf_sent_more = tfidf_vectorizer.fit_transform(df['sent_more'].fillna(''))
tfidf_sent_less = tfidf_vectorizer.fit_transform(df['sent_less'].fillna(''))

print("En Ã§ok geÃ§en kelimeler (Ã–nyargÄ±lÄ± CÃ¼mleler):")
print(pd.DataFrame(tfidf_sent_more.toarray(), columns=tfidf_vectorizer.get_feature_names_out()).sum().sort_values(ascending=False).head(10))

print("En Ã§ok geÃ§en kelimeler (Ã–nyargÄ±sÄ±z CÃ¼mleler):")
print(pd.DataFrame(tfidf_sent_less.toarray(), columns=tfidf_vectorizer.get_feature_names_out()).sum().sort_values(ascending=False).head(10))

# PCA GÃ¶rselleÅŸtirme
pca = PCA(n_components=2)
reduced_embeddings = pca.fit_transform(tfidf_sent_more.toarray())

plt.figure(figsize=(10, 8))
plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], alpha=0.7, cmap='viridis')
plt.title('TF-IDF Kelime UzayÄ± PCA GÃ¶rselleÅŸtirmesi')
plt.show()

# TemizlenmiÅŸ Veriyi Kaydet
output_file = '/Users/bilge/Desktop/crows-pairs/data/cleaned_crows_pairs.csv'
df.to_csv(output_file, index=False)
print(f"\n TemizlenmiÅŸ veri baÅŸarÄ±yla kaydedildi: {output_file}")

print("\n Analiz baÅŸarÄ±yla tamamlandÄ±!")
